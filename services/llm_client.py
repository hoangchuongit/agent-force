
import time
from openai import OpenAI, OpenAIError
from config import OPENAI_MODEL, TEMPERATURE

class OpenAIClient:
    """
    LLM client chu·∫©n cho d·ª± √°n Crisis Response
    - H·ªó tr·ª£ chat() sync
    - H·ªó tr·ª£ chat_stream() async generator
    - D√πng config.py ƒë·ªÉ load c·∫•u h√¨nh model
    """

    def __init__(self, model: str | None = None, temperature: float | None = None):
        self.model = model or OPENAI_MODEL
        self.temperature = temperature if temperature is not None else TEMPERATURE
        self.client = OpenAI()

    def chat(self, prompt: str) -> str:
        """G·ª≠i prompt v√† tr·∫£ v·ªÅ chu·ªói ph·∫£n h·ªìi ƒë·∫ßy ƒë·ªß."""
        try:
            print(f"üîç [Prompt g·ª≠i GPT]: {prompt[:120]}‚Ä¶")
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
            )
            content = resp.choices[0].message.content
            time.sleep(1.0)  # m√¥ ph·ªèng ƒë·ªô tr·ªÖ th·ª±c t·∫ø
            return content.strip()
        except OpenAIError as e:
            print(f"‚ö†Ô∏è L·ªói GPT: {e}")
            return "T√¥i g·∫∑p ch√∫t s·ª± c·ªë khi ph·∫£n h·ªìi."

    async def chat_stream(self, prompt: str):
        """Tr·∫£ v·ªÅ ph·∫£n h·ªìi theo d√≤ng (d·∫°ng async generator)"""
        try:
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.temperature,
                stream=True,
            )
            for chunk in stream:
                delta = chunk.choices[0].delta
                if delta.content:
                    yield delta.content
        except OpenAIError as e:
            yield f"[L·ªñI khi stream GPT]: {e}"
